{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4XOdlbe5xPe",
    "outputId": "e5df0067-0fa7-4827-e396-0a99400be030"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import string\n",
    "from abc import ABC\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer,\n",
    "                          LogitsProcessor, LogitsProcessorList)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NLTK Import\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kbpq_genk2C"
   },
   "source": [
    "### Load LogitsProcessor for Beam Search Constrainment\n",
    "\n",
    "We implement the LogitsProcessor class to get our desired effect. Our custom class should implement the __call__ method of LogitsProcessor.\n",
    "\n",
    "This method will be called during each step of the beam search algorithm. The method takes as input the input_ids sequence of the partially generated beam and the scores of the next possible tokens.\n",
    "\n",
    "By manipulating these scores based on the tokens present in the input_ids, we can control the structure of the generated sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SZI3CKIknk2E"
   },
   "outputs": [],
   "source": [
    "class WordValidator(ABC):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class BannedWords(WordValidator):\n",
    "    def __init__(self, dictionary):\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def is_valid_word(self, word, input_idx, beam_sequence, beam_scores):\n",
    "        return word not in self.dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "z2ps0VnEnk2F"
   },
   "outputs": [],
   "source": [
    "SPLIT_WORD_TOKENS = {\n",
    "    ' ',\n",
    "    '.',\n",
    "    ',',\n",
    "    '_',\n",
    "    '?',\n",
    "    '!',\n",
    "}\n",
    "\n",
    "class ConsistentLogitsProcessor(LogitsProcessor):\n",
    "    r\"\"\"\n",
    "    [`ConsistentLogitsProcessor`] enforcing constraints from source documentation on logits\n",
    "    Args:\n",
    "        min_length (`int`):\n",
    "            The minimum length below which the score of `eos_token_id` is set to `-float(\"Inf\")`.\n",
    "        eos_token_id (`int`):\n",
    "            The id of the *end-of-sequence* token.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, num_beams, word_validator: WordValidator):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.word_validator = word_validator\n",
    "        self.num_beams = num_beams\n",
    "        self.excluded_beams_by_input_idx = defaultdict(list)\n",
    "        self.words_to_check_by_input_idx = defaultdict(lambda: 0)\n",
    "        self.failed_sequences = set()\n",
    "\n",
    "    def is_valid_beam(\n",
    "        self,\n",
    "        input_idx, # input idx being processed\n",
    "        sequence,  # sequence generated so far\n",
    "        token_id,  # next token to be generated\n",
    "        beam_scores,  # probability of all tokens to be generated\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Check whether the beam is valid. This method backtracks to confirm\n",
    "        words are valid when it detects the predicted suword (token) is a\n",
    "        word ending\n",
    "        \"\"\"\n",
    "        \n",
    "        current_subword = self.tokenizer.decode(token_id)\n",
    "        backtrack_word = \"\"\n",
    "        is_subword_ending = False\n",
    "        for char in current_subword:\n",
    "            if char in SPLIT_WORD_TOKENS:\n",
    "                is_subword_ending = True\n",
    "                break\n",
    "            else:\n",
    "                backtrack_word += char\n",
    "        \n",
    "        backtrack_done = False\n",
    "        if is_subword_ending:\n",
    "            prev_subword_idx = len(sequence) - 1\n",
    "            while prev_subword_idx != 0 and not backtrack_done:\n",
    "                prev_token_id = sequence[prev_subword_idx]\n",
    "                prev_subword = self.tokenizer.decode(prev_token_id)\n",
    "                prev_char_idx = len(prev_subword) - 1\n",
    "                while prev_char_idx >= 0:\n",
    "                    prev_char = prev_subword[prev_char_idx]\n",
    "                    if prev_char not in SPLIT_WORD_TOKENS:\n",
    "                        backtrack_word = prev_char + backtrack_word\n",
    "                    else:\n",
    "                        backtrack_done = True\n",
    "                        break \n",
    "                    prev_char_idx -= 1\n",
    "                prev_subword_idx -= 1\n",
    "            self.words_to_check_by_input_idx[input_idx] += 1\n",
    "            # Call validator to check whether the word is valid\n",
    "            if not self.word_validator.is_valid_word(\n",
    "                backtrack_word,\n",
    "                input_idx, \n",
    "                sequence, \n",
    "                beam_scores\n",
    "            ):\n",
    "                return False\n",
    "        return True\n",
    "            \n",
    "                \n",
    "\n",
    "    def __call__(\n",
    "        self, input_ids: torch.LongTensor, scores: torch.FloatTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "        blocked_beams_by_input_idx = defaultdict(lambda: 0)\n",
    "        # for every beam (partially generated sentence)\n",
    "        for beam_idx, (beam_input_ids, beam_scores) in enumerate(\n",
    "            zip(input_ids, scores)\n",
    "        ):\n",
    "            top_k = beam_scores.topk(k=5)\n",
    "            for prob, idx in zip(top_k[0], top_k[1]):\n",
    "                input_idx = beam_idx // self.num_beams\n",
    "                if not self.is_valid_beam(\n",
    "                    input_idx, beam_input_ids, idx.item(), scores[beam_idx]\n",
    "                ):\n",
    "                    scores[beam_idx, :] = -float(\"inf\")\n",
    "                    self.excluded_beams_by_input_idx[input_idx].append((\n",
    "                        beam_input_ids,\n",
    "                        idx.item(),\n",
    "                        prob.item(),\n",
    "                    ))\n",
    "                    blocked_beams_by_input_idx[input_idx] += 1\n",
    "        \n",
    "        for input_idx, n_blocked in blocked_beams_by_input_idx.items():\n",
    "            if n_blocked == self.num_beams:\n",
    "                self.failed_sequences.add(input_idx)\n",
    "                    \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3MfwYmFGnk2I"
   },
   "outputs": [],
   "source": [
    "def entropy(p_dist: torch.Tensor) -> float:\n",
    "    \"\"\" \"\n",
    "    Calculates Shannon entropy for a probability distribution\n",
    "    Args:\n",
    "        p_dist: probability distribution (torch.Tensor)\n",
    "    Returns:\n",
    "        entropy (float)\n",
    "    \"\"\"\n",
    "    # add epsilon because log(0) = nan\n",
    "    p_dist = p_dist.view(-1) + 1e-12\n",
    "    return -torch.mul(p_dist, p_dist.log()).sum(0).item()\n",
    "\n",
    "\n",
    "def generate_summaries_with_constraints(\n",
    "    model: AutoModelForSeq2SeqLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    docs_to_summarize: List[str],\n",
    "    word_validator: WordValidator,\n",
    "    num_beams: int = 4,\n",
    "    max_length: int = 150,\n",
    "    return_beam_metadata: bool = False,\n",
    "    device: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    inputs = tokenizer(\n",
    "        docs_to_summarize,\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "    )\n",
    "    input_token_ids = inputs.input_ids.to(device)\n",
    "    consistency_forced = ConsistentLogitsProcessor(\n",
    "        tokenizer,\n",
    "        num_beams,\n",
    "        word_validator\n",
    "    )\n",
    "    model_output = model.generate(\n",
    "        input_token_ids,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        # remove_invalid_values=True,\n",
    "        logits_processor=LogitsProcessorList([consistency_forced]),\n",
    "        max_length=max_length,\n",
    "    )\n",
    "\n",
    "    generated_summaries = [\n",
    "        (\n",
    "            tokenizer.decode(\n",
    "                ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            if (\n",
    "                consistency_forced is None\n",
    "                or idx not in consistency_forced.failed_sequences\n",
    "            )\n",
    "            else \"<Failed generation: blocked all beams>\"\n",
    "        )\n",
    "        for idx, ids in enumerate(model_output.sequences)\n",
    "    ]\n",
    "    \n",
    "    if not return_beam_metadata:\n",
    "        return generated_summaries\n",
    "    \n",
    "    else:\n",
    "        # reshape model_output scores to (n_seqs x seq len x n_beams x vocab)\n",
    "        model_beam_scores = (\n",
    "            torch.stack(model_output.scores)\n",
    "            .reshape(len(model_output.scores), len(generated_summaries), num_beams, -1)\n",
    "            .permute(1, 0, 2, 3)\n",
    "        )\n",
    "        # Collect Beam Search Metadata\n",
    "        beams_metadata = []\n",
    "        if model_output.beam_indices is not None:\n",
    "            for seq_idx in range(model_output.sequences.shape[0]):\n",
    "                top_beam_indices = [x.item() for x in model_output.beam_indices[seq_idx]]\n",
    "                seq_beams = {\n",
    "                    \"beams\": [list() for _ in range(num_beams)],\n",
    "                    \"selected_beam_indices\": top_beam_indices,\n",
    "                    \"dropped_seqs\": consistency_forced.excluded_beams_by_input_idx[seq_idx],\n",
    "                    \"n_words_checked\": consistency_forced.words_to_check_by_input_idx[seq_idx],\n",
    "                }\n",
    "                beams_metadata.append(seq_beams)\n",
    "\n",
    "                for idx, output_token_id in enumerate(model_output.sequences[seq_idx][1:]):\n",
    "                    for beam_idx in range(num_beams):\n",
    "                        beam_probs = torch.exp(model_beam_scores[seq_idx][idx][beam_idx])\n",
    "                        beam_top_alternatives = []\n",
    "                        top_probs = torch.topk(beam_probs, k=num_beams)\n",
    "                        for i, v in zip(top_probs.indices, top_probs.values):\n",
    "                            beam_top_alternatives.append(\n",
    "                                {\n",
    "                                    \"token\": tokenizer.decode(i),\n",
    "                                    \"token_id\": i.item(),\n",
    "                                    \"probability\": v.item(),\n",
    "                                }\n",
    "                            )\n",
    "                        seq_beams[\"beams\"][beam_idx].append(\n",
    "                            {\n",
    "                                \"top_tokens\": beam_top_alternatives,\n",
    "                                \"entropy\": entropy(beam_probs),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        return generated_summaries, beams_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5RbKxqVnk2K"
   },
   "source": [
    "### Generate a summary without beam search constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JV4FPtWMnk2M"
   },
   "outputs": [],
   "source": [
    "def generate_summaries_without_constraints(\n",
    "    model: AutoModelForSeq2SeqLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    docs_to_summarize: List[str],\n",
    "    num_beams: int = 4,\n",
    "    max_length: int = 150,\n",
    "    device: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "):\n",
    "    inputs = tokenizer(docs_to_summarize, max_length=1024, return_tensors='pt', truncation=True,)\n",
    "    ids = model.generate(inputs['input_ids'].to(device), num_beams=num_beams, max_length=max_length, early_stopping=True)\n",
    "    summaries = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in ids]\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gkD8uUAnk2O"
   },
   "source": [
    "### Load HPI Summarization Models\n",
    "The first sentence of the hospital course section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dhPfSrZSnk2P"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/hpi-dataset/summarization-datasets/HPI_TEST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SzBRzn5xnk2Q"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(path):\n",
    "    return (\n",
    "        AutoModelForSeq2SeqLM.from_pretrained(path),\n",
    "        AutoTokenizer.from_pretrained(path),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "DaKnIdwCnk2Q"
   },
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer('../models/HPI-BART')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h97J1coznk2R"
   },
   "source": [
    "### Create a Medical Dictionary from SNOMED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Q108J-JQnk2S"
   },
   "outputs": [],
   "source": [
    "def to_words(text):\n",
    "    res = re.findall(r'\\b[^\\d\\W]+\\b', text)\n",
    "    return [x.lower() for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "8xP1XntmpjW4"
   },
   "outputs": [],
   "source": [
    "with open('../data/snomed/medical_dictionary.txt', 'r',encoding=\"ISO-8859-1\") as fd:\n",
    "    medical_words = []\n",
    "    for row in fd:\n",
    "        medical_words.append(row.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-EMn2LRcnk2W",
    "outputId": "cb58d738-4614-4345-e32f-aadaf29c32d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79521"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(medical_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31sL83etnk2X"
   },
   "source": [
    "### Set HPI summary we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "f6raurDjnk2X"
   },
   "outputs": [],
   "source": [
    "text = df.loc[2,\"TEXT\"] #14 is a good example\n",
    "true_summary = df.loc[2,\"SUMMARY\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "NU4nhBaink2X",
    "outputId": "678737e0-3f67-4d10-8d4b-b75e77f185e6",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CATEGORY: Physician Attending Admission Note - MICU, TYPE: EMERGENCY, ADMIT LOCATION: EMERGENCY ROOM ADMIT, AGE: 81, GENDER: M, MARITAL STATUS: WIDOWED, ETHNICITY: WHITE, ADMIT DX: LOWER GI BLEED, ADMIT TEXT: 81 yr old man with remote hx of colon ca, diverticulitis, presents with 4 days of BRBPR and LLQ pain. This AM home health aide noted blood in stool. In ED afeb 129/83 but HR 100s rectal BRB Abd'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXOGAtdrnk2Y"
   },
   "source": [
    "### Set Banned Words as Medical Words Not in Source Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "S4UYoLZ4nk2Y"
   },
   "outputs": [],
   "source": [
    "words_in_text = list(set(to_words(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4u5NT0rNnk2Z"
   },
   "outputs": [],
   "source": [
    "medical_in_text = [x for x in medical_words if x in words_in_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lsf0i1bfnk2Z",
    "outputId": "aedd8f9b-3bf5-4720-b424-f9c032b01c35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(medical_in_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VL8NEyCHnk2Z",
    "outputId": "0ff18a30-9e59-41b5-fa50-761aecf06246"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['diverticulitis', 'colon', 'brb', 'brbpr', 'rectal']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medical_in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oEo7UXvpnk2a",
    "outputId": "1296d51f-5467-43d9-8510-c0c32d5519ab"
   },
   "outputs": [],
   "source": [
    "# allow synonyms from NLTK wordnet\n",
    "medical_synonyms_in_text = []\n",
    "for word in medical_in_text:\n",
    "  for syn in wordnet.synsets(word):\n",
    "    for i in syn.lemmas():\n",
    "      medical_synonyms_in_text.append(i.name())\n",
    "medical_synonyms_in_text = medical_in_text + list(set(medical_synonyms_in_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "3PQj6yIsnk2a"
   },
   "outputs": [],
   "source": [
    "banned_words = BannedWords(set([x for x in medical_words if x not in medical_synonyms_in_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEa0YuJRsgHL",
    "outputId": "ba54369a-7dbe-4ef9-d0b5-6b99657984ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79516\n"
     ]
    }
   ],
   "source": [
    "print(len(banned_words.dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjoFzy1ink2b"
   },
   "source": [
    "### Run Summarization with Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-qbj26uKnk2b"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "pred_summaries_notconstrained = generate_summaries_without_constraints(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    [text],\n",
    "    num_beams=6,\n",
    "    max_length=150\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J-iTUk91nk2b",
    "outputId": "b3d1fcb8-3f67-416a-dd23-b51624718900"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "pred_summaries_constrained, metadata_constrained = generate_summaries_with_constraints(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    [text],\n",
    "    word_validator = banned_words,\n",
    "    num_beams=6,\n",
    "    max_length = 150,\n",
    "    return_beam_metadata=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "jk8q9H-Knk2b",
    "outputId": "c58864fa-6016-483f-b6ed-abdcd71b30c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Acute blood loss anemia [**1-9**] GIB: The patient was initially admitted to the MICU for a GI bleed.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m24_82c5nk2c",
    "outputId": "66132013-251f-4757-dbb8-08fdb8af8d84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acute Blood Loss Anemia due to GI Bleeding due to Duodenal Ulcer - GI Consultation was obtained.Patient was initially admitted to the MICU and given his history of BRBPR and diverticulitis, was taken to the']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_summaries_notconstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXYK0MfsGpGJ",
    "outputId": "d16b9c4c-58e3-4426-abee-51dd356f784a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acute Blood Loss Anemia due to GI Bleeding due to Duodenal Ulcers: The patient was initially admitted with blood in the rectal BRBPR and had an upper GI Bleed which was treated with Vaseline and a band anast']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_summaries_constrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wtkShQPnk2c",
    "outputId": "24db5ee1-5842-499f-d932-1e33e08f117e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE NOT CONSTRAINED:\n",
      "ROUGE1\":  0.737\n",
      "ROUGE2\":  0.556\n",
      "ROUGEL\":  0.579\n",
      "/n\n",
      "ROUGE CONSTRAINED:\n",
      "ROUGE1\":  0.737\n",
      "ROUGE2\":  0.444\n",
      "ROUGEL\":  0.632\n"
     ]
    }
   ],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "scores_nc = scorer.score(true_summary, pred_summaries_notconstrained[0])\n",
    "scores_c = scorer.score(true_summary, pred_summaries_constrained[0])\n",
    "print(\"ROUGE NOT CONSTRAINED:\")\n",
    "print('ROUGE1\": ',round(scores_nc['rouge1'][1],3))\n",
    "print('ROUGE2\": ',round(scores_nc['rouge2'][1],3))\n",
    "print('ROUGEL\": ',round(scores_nc['rougeL'][1],3))\n",
    "print(\"/n\")\n",
    "print(\"ROUGE CONSTRAINED:\")\n",
    "print('ROUGE1\": ',round(scores_c['rouge1'][1],3))\n",
    "print('ROUGE2\": ',round(scores_c['rouge2'][1],3))\n",
    "print('ROUGEL\": ',round(scores_c['rougeL'][1],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NoSoOy5Onk2c"
   },
   "source": [
    "### RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "UvwcihH8gO45"
   },
   "outputs": [],
   "source": [
    "def add_breaks(text):\n",
    "  return re.sub(\"(.{50})\", \"\\\\1\\n\", text, 0, re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"../results\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UACIn8lDnk2d",
    "outputId": "92d26779-7b79-4dbe-eedf-d8d9cad96c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 560 summaries processed.\n",
      "10 of 560 summaries processed.\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "rouge_scores_nc = {'r1': [], 'r2': [], 'rL': []}\n",
    "rouge_scores_c = {'r1': [], 'r2': [], 'rL': []}\n",
    "\n",
    "thrown_out = 0\n",
    "\n",
    "for id in range(0, len(df)):\n",
    "    text = df.loc[id,\"TEXT\"]\n",
    "    true_summary = df.loc[id,\"SUMMARY\"]\n",
    "    words_in_text = list(set(to_words(text)))\n",
    "    medical_in_text = [x for x in medical_words if x in words_in_text]\n",
    "\n",
    "    # allow synonyms from NLTK wordnet\n",
    "    medical_synonyms_in_text = []\n",
    "    for word in medical_in_text:\n",
    "      for syn in wordnet.synsets(word):\n",
    "        for i in syn.lemmas():\n",
    "          medical_synonyms_in_text.append(i.name())\n",
    "    medical_synonyms_in_text = medical_in_text + list(set(medical_synonyms_in_text))\n",
    "    \n",
    "    banned_words = BannedWords(set([x for x in medical_words if x not in medical_synonyms_in_text]))\n",
    "    \n",
    "    pred_summaries_notconstrained = generate_summaries_without_constraints(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        [text],\n",
    "        num_beams=6,\n",
    "        max_length=150\n",
    "    )\n",
    "    \n",
    "    pred_summaries_constrained, metadata_constrained = generate_summaries_with_constraints(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        [text],\n",
    "        word_validator = banned_words,\n",
    "        num_beams=6,\n",
    "        max_length = 150,\n",
    "        return_beam_metadata=True\n",
    "    )\n",
    "    \n",
    "    if id % 10 == 0:\n",
    "        print(str(id) + \" of \" + str(len(df)) + \" summaries processed.\")\n",
    "    \n",
    "    if pred_summaries_constrained[0] != \"<Failed generation: blocked all beams>\":\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1','rouge2','rougeL'], use_stemmer=True)\n",
    "        scores_nc = scorer.score(true_summary, pred_summaries_notconstrained[0])\n",
    "        scores_c = scorer.score(true_summary, pred_summaries_constrained[0])\n",
    "        rouge_scores_nc['r1'].append(scores_nc['rouge1'][1])\n",
    "        rouge_scores_nc['r2'].append(scores_nc['rouge2'][1])\n",
    "        rouge_scores_nc['rL'].append(scores_nc['rougeL'][1])\n",
    "        rouge_scores_c['r1'].append(scores_c['rouge1'][1])\n",
    "        rouge_scores_c['r2'].append(scores_c['rouge2'][1])\n",
    "        rouge_scores_c['rL'].append(scores_c['rougeL'][1])\n",
    "\n",
    "        #save summaries\n",
    "        text_file = 'ADMISSION NOTE:\\n'\n",
    "        text_file += add_breaks(text)\n",
    "        text_file += '\\n\\nGOLD TRUTH SUMMARY:\\n'\n",
    "        text_file += add_breaks(true_summary)\n",
    "        text_file += '\\n\\nNON-CONSTRAINED SUMMARY:\\n'\n",
    "        text_file += add_breaks(pred_summaries_notconstrained[0])\n",
    "        text_file += '\\n\\nCONSTRAINED SUMMARY:\\n'\n",
    "        text_file += add_breaks(pred_summaries_constrained[0])\n",
    "\n",
    "        file_path = \"../results/\"\n",
    "        file_path += \"file\" + str(id + 1) + \".txt\"\n",
    "        with open(file_path, \"w\") as f:\n",
    "          f.write(text_file)\n",
    "\n",
    "    else:\n",
    "        thrown_out += 1\n",
    "print(\"Summaries thrown out: \" + str(thrown_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VZlAf-Vjnk2d",
    "outputId": "709f46c2-70fe-42f3-f281-3dc32da060e8"
   },
   "outputs": [],
   "source": [
    "print(\"ROUGE NOT CONSTRAINED:\")\n",
    "print('ROUGE1\": ',round(statistics.mean(rouge_scores_nc['r1']),3))\n",
    "print('ROUGE2\": ',round(statistics.mean(rouge_scores_nc['r2']),3))\n",
    "print('ROUGEL\": ',round(statistics.mean(rouge_scores_nc['rL']),3))\n",
    "print(\"/n\")\n",
    "print(\"ROUGE CONSTRAINED:\")\n",
    "print('ROUGE1\": ',round(statistics.mean(rouge_scores_c['r1']),3))\n",
    "print('ROUGE2\": ',round(statistics.mean(rouge_scores_c['r2']),3))\n",
    "print('ROUGEL\": ',round(statistics.mean(rouge_scores_c['rL']),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEoXaePSnk2d"
   },
   "outputs": [],
   "source": [
    "# Examples\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "hpi_summarization_with_beam_search_constrained.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "consistency",
   "language": "python",
   "name": "consistency"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
